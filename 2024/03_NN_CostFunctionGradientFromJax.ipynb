{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A simple neural network and cost function gradient via jax"
      ],
      "metadata": {
        "id": "OAaBOgjUps0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FlorianMarquardt/machine-learning-for-physicists/blob/master/2024/03_NN_CostFunctionGradientFromJax.ipynb)"
      ],
      "metadata": {
        "id": "wR3pRGrCrq6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
        "\n",
        "Lecture 3\n",
        "\n",
        "See https://machine-learning-for-physicists.org and the current course website linked there!\n",
        "\n",
        "This notebook shows how to build a little network and evaluate the gradient of the cost function using jax, and how to apply one step of gradient descent.\n",
        "\n",
        "MIT License."
      ],
      "metadata": {
        "id": "KGjabluhpdBH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CFxxh7Y0g1yr"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple function, to show the principles of jax grad:\n",
        "def f(x):\n",
        "  return jnp.sum(x**2)"
      ],
      "metadata": {
        "id": "Y2H0vWx8qy0w"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use jax autodifferentiation to get gradient of f\n",
        "# with respect to the input vector x\n",
        "grad_f=grad(f)"
      ],
      "metadata": {
        "id": "t9kGPerBq3zU"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show what happens in a numerical example:\n",
        "x=jnp.array([0.1,0.2,0.3])\n",
        "\n",
        "print(\"value of f: \", f(x))\n",
        "print(\"value of grad f:\", grad_f(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcvvmGL3q6vT",
        "outputId": "aa856410-9b32-4cee-8190-65f2f3e306d2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value of f:  0.14\n",
            "value of grad f: [0.2 0.4 0.6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now define a simple neural network\n",
        "# (arbitrary number of layers, but relu activation at each layer)\n",
        "def network(parameters,x):\n",
        "  \"\"\"\n",
        "  Evaluate network.\n",
        "\n",
        "  parameters=[[weights1,biases1],[weights2,biases2],...]\n",
        "  x=input vector\n",
        "  \"\"\"\n",
        "  for weights,biases in parameters:\n",
        "    # weights has shape (neurons_lower_layer,neurons_upper_layer),\n",
        "    # biases has shape (neurons_upper_layer,)\n",
        "    z=jnp.dot(x,weights)+biases\n",
        "    x=(z>0)*z # relu activation\n",
        "  return x"
      ],
      "metadata": {
        "id": "U-95MFmAg6A3"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our network has structure 2 (input) -- 3 -- 1 (output)\n",
        "weights1=jnp.array([[0.1,0.3,0.5],[-0.4,0.2,0.8]]) # shape (2,3)\n",
        "biases1=jnp.array([0.1,-0.2,0.3]) # shape (3,)\n",
        "weights2=jnp.array([[0.2],[0.7],[-0.5]]) # shape (3,1)\n",
        "biases2=jnp.array([0.2]) # shape (1,)\n",
        "\n",
        "params=[[weights1,biases1],[weights2,biases2]]"
      ],
      "metadata": {
        "id": "seLhYn-Ch-xc"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply network to test input\n",
        "x=jnp.array([0.9,-0.5])\n",
        "print(network(params,x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PK5p7hfjLN_",
        "outputId": "da52f343-ece3-41a9-a12d-2ca89fc4e220"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.10300001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a cost function (here: quadratic deviation)\n",
        "def cost(params,x,y_target):\n",
        "  return jnp.sum( ( network(params,x) - y_target )**2 )\n",
        "# note: we would divide by the batch size jnp.shape(x)[0] if we want to average\n",
        "# over a batch (but right now we do not do batches)"
      ],
      "metadata": {
        "id": "CcDF6tb8kb0R"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost(params,x,1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuWFKRvClNHU",
        "outputId": "da5b6ba5-b10e-4a15-9f82-09c79b8deb7c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(0.804609, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now apply jax autodifferentiation to get the\n",
        "# gradient of the cost function with respect to the params:\n",
        "grad_cost=grad(cost,argnums=0) # argnums=0 means first argument, i.e. params"
      ],
      "metadata": {
        "id": "d6BH6ti5jbSD"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the gradient of the cost function,\n",
        "# at the current values of the parameters 'params',\n",
        "# and for that given x input vector (and with y_target==1.0):\n",
        "grad_cost(params,x,1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH9MHw2XkV5L",
        "outputId": "4ec0077b-87e7-4d25-f74e-017c76b3bfbc"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Array([[-0.32292002, -0.        ,  0.8073    ],\n",
              "         [ 0.17940001,  0.        , -0.4485    ]], dtype=float32),\n",
              "  Array([-0.35880002, -0.        ,  0.897     ], dtype=float32)],\n",
              " [Array([[-0.69966],\n",
              "         [-0.     ],\n",
              "         [-0.6279 ]], dtype=float32),\n",
              "  Array([-1.794], dtype=float32)]]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output is just the same shape as params!\n",
        "Each component is the gradient of the cost function with respect to that component!"
      ],
      "metadata": {
        "id": "t5tMeWOopSeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store this whole nested list:\n",
        "grad_value = grad_cost(params,x,1.0)"
      ],
      "metadata": {
        "id": "pzEr1eCJlhSj"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now update all parameters according to the negative gradient!\n",
        "learning_rate=0.1\n",
        "\n",
        "new_params=[(weights-learning_rate*grad_weights,biases-learning_rate*grad_biases) for\n",
        "            (weights,biases),(grad_weights,grad_biases) in zip(params,grad_value)]"
      ],
      "metadata": {
        "id": "DNNZ3m3Tlopi"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print new parameters:\n",
        "new_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPxZr1_YmVQl",
        "outputId": "6a73fec6-4bb0-4076-efc3-541ceea46218"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Array([[ 0.132292  ,  0.3       ,  0.41927   ],\n",
              "         [-0.41794002,  0.2       ,  0.84485   ]], dtype=float32),\n",
              "  Array([ 0.13588001, -0.2       ,  0.2103    ], dtype=float32)),\n",
              " (Array([[ 0.269966],\n",
              "         [ 0.7     ],\n",
              "         [-0.43721 ]], dtype=float32),\n",
              "  Array([0.3794], dtype=float32))]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cost should have gone down!\n",
        "cost(new_params,x,1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3d0QQW2mSwz",
        "outputId": "957c932e-be86-4261-afdc-1c565d16da67"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(0.32216328, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# jax makes such things more convenient:\n",
        "from jax.tree_util import tree_map\n",
        "\n",
        "new_params = tree_map(lambda x,y: x - learning_rate * y, params, grad_value)"
      ],
      "metadata": {
        "id": "hQrINMERmazO"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# should be the same result as above:\n",
        "new_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n8iHVGTnD4Z",
        "outputId": "043b4665-e29c-4da2-ba16-2c2edd925cf7"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Array([[ 0.132292  ,  0.3       ,  0.41927   ],\n",
              "         [-0.41794002,  0.2       ,  0.84485   ]], dtype=float32),\n",
              "  Array([ 0.13588001, -0.2       ,  0.2103    ], dtype=float32)],\n",
              " [Array([[ 0.269966],\n",
              "         [ 0.7     ],\n",
              "         [-0.43721 ]], dtype=float32),\n",
              "  Array([0.3794], dtype=float32)]]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train neural networks using jax and simple\n",
        "stochastic gradient descent! Have fun!\n",
        "\n",
        "(no flax or optax or anything else needed at this point!)"
      ],
      "metadata": {
        "id": "difGqMG4pO9C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CsA2CZaEnEpp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}