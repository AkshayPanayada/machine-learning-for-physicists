{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A few more jax tricks (compile and batches)"
      ],
      "metadata": {
        "id": "OAaBOgjUps0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FlorianMarquardt/machine-learning-for-physicists/blob/master/2024/03_MoreJaxTricks.ipynb)"
      ],
      "metadata": {
        "id": "wR3pRGrCrq6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
        "\n",
        "Lecture 3\n",
        "\n",
        "See https://machine-learning-for-physicists.org and the current course website linked there!\n",
        "\n",
        "This notebook shows how to build a little network and evaluate the gradient of the cost function using jax, and how to apply one step of gradient descent.\n",
        "\n",
        "MIT License."
      ],
      "metadata": {
        "id": "KGjabluhpdBH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CFxxh7Y0g1yr"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, value_and_grad, jit, vmap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now define a simple neural network\n",
        "# (arbitrary number of layers, but relu activation at each layer)\n",
        "def network(parameters,x):\n",
        "  \"\"\"\n",
        "  Evaluate network.\n",
        "\n",
        "  parameters=[[weights1,biases1],[weights2,biases2],...]\n",
        "  x=input vector\n",
        "  \"\"\"\n",
        "  for weights,biases in parameters:\n",
        "    # weights has shape (neurons_lower_layer,neurons_upper_layer),\n",
        "    # biases has shape (neurons_upper_layer,)\n",
        "    z=jnp.dot(x,weights)+biases\n",
        "    x=(z>0)*z # relu activation\n",
        "  return x"
      ],
      "metadata": {
        "id": "U-95MFmAg6A3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our network has structure 2 (input) -- 3 -- 1 (output)\n",
        "weights1=jnp.array([[0.1,0.3,0.5],[-0.4,0.2,0.8]]) # shape (2,3)\n",
        "biases1=jnp.array([0.1,-0.2,0.3]) # shape (3,)\n",
        "weights2=jnp.array([[0.2],[0.7],[-0.5]]) # shape (3,1)\n",
        "biases2=jnp.array([0.2]) # shape (1,)\n",
        "\n",
        "params=[[weights1,biases1],[weights2,biases2]]"
      ],
      "metadata": {
        "id": "seLhYn-Ch-xc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply network to test input\n",
        "x=jnp.array([0.9,-0.5])\n",
        "print(network(params,x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PK5p7hfjLN_",
        "outputId": "c2123f82-9fbb-445d-c575-6056a595ae2a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.10300001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a cost function (here: quadratic deviation)\n",
        "def cost(params,x,y_target):\n",
        "  return jnp.sum( ( network(params,x) - y_target )**2 )\n",
        "# note: we would divide by the batch size jnp.shape(x)[0] if we want to average\n",
        "# over a batch (but right now we do not do batches)"
      ],
      "metadata": {
        "id": "CcDF6tb8kb0R"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost(params,x,1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuWFKRvClNHU",
        "outputId": "11571ddd-ed17-486d-9c04-fa4b4b8500d9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(0.804609, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "cost(params,x,1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE19kgIhW5_q",
        "outputId": "71e8a031-4495-47f3-d0d2-eea73f3ed59f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "194 µs ± 25 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile using jax \"jit\" (just-in-time compilation)"
      ],
      "metadata": {
        "id": "2aPBxbVxbgSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a cost function (here: quadratic deviation)\n",
        "# compile it!\n",
        "@jit\n",
        "def compiled_cost(params,x,y_target):\n",
        "  return jnp.sum( ( network(params,x) - y_target )**2 )\n",
        "# note: we would divide by the batch size jnp.shape(x)[0] if we want to average\n",
        "# over a batch (but right now we do not do batches)"
      ],
      "metadata": {
        "id": "zmflN5_AXDOC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "compiled_cost(params,x,1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZdIsrDjXIly",
        "outputId": "be4613bb-f587-48c3-e113-833afebe1c69"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.8 µs ± 2.17 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now apply jax autodifferentiation to get the\n",
        "# gradient of the cost function with respect to the params:\n",
        "# note: it is enough to apply jit once at the very end!\n",
        "grad_cost=jit(grad(cost,argnums=0)) # argnums=0 means first argument, i.e. params"
      ],
      "metadata": {
        "id": "d6BH6ti5jbSD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "grad_cost(params,x,1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doWbBa0EX0t-",
        "outputId": "e990eecd-3744-4b53-ff84-c2103f363d6c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15.9 µs ± 2.69 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the gradient of the cost function,\n",
        "# at the current values of the parameters 'params',\n",
        "# and for that given x input vector (and with y_target==1.0):\n",
        "grad_value = grad_cost(params,x,1.0)"
      ],
      "metadata": {
        "id": "kH9MHw2XkV5L"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output is just the same shape as params!\n",
        "Each component is the gradient of the cost function with respect to that component!"
      ],
      "metadata": {
        "id": "t5tMeWOopSeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# update parameters:\n",
        "from jax.tree_util import tree_map\n",
        "\n",
        "learning_rate=0.1\n",
        "new_params = tree_map(lambda x,y: x - learning_rate * y, params, grad_value)"
      ],
      "metadata": {
        "id": "hQrINMERmazO"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n8iHVGTnD4Z",
        "outputId": "dedae900-a62d-4631-8e3a-0573efd2aaf3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Array([[ 0.132292  ,  0.3       ,  0.41927   ],\n",
              "         [-0.41794002,  0.2       ,  0.84485   ]], dtype=float32),\n",
              "  Array([ 0.13588001, -0.2       ,  0.2103    ], dtype=float32)],\n",
              " [Array([[ 0.269966],\n",
              "         [ 0.7     ],\n",
              "         [-0.43721 ]], dtype=float32),\n",
              "  Array([0.3794], dtype=float32)]]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train neural networks using jax and simple\n",
        "stochastic gradient descent! Have fun!\n",
        "\n",
        "(no flax or optax or anything else needed at this point!)"
      ],
      "metadata": {
        "id": "difGqMG4pO9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# even better: get both value and gradient at the same time (more efficient):\n",
        "value_and_grad_cost=jit(value_and_grad(cost,argnums=0))"
      ],
      "metadata": {
        "id": "CsA2CZaEnEpp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_value,grad_value = value_and_grad_cost(params,x,1.0)"
      ],
      "metadata": {
        "id": "vyrhwD1rYhBR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igfe_XtBYmLI",
        "outputId": "519b00a7-c6eb-43ab-f63b-13e215502d98"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(0.804609, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch processing (using jax vmap)"
      ],
      "metadata": {
        "id": "YMEEpc_Sbx5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now also introduce batch processing! We now want shape of x to be (batchsize,num_input_neurons). We do not want to rewrite our network or cost function!"
      ],
      "metadata": {
        "id": "alb0KdvSb2sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vmap does the trick!\n",
        "# It produces a new version of any function, but now with\n",
        "# a batch index (vectorized processing)!\n",
        "\n",
        "# in_axes says which index is the batch index, for each\n",
        "# of the arguments of the function. 'params' (the first argument)\n",
        "# does not have any batch index, therefore we write 'None':\n",
        "batched_cost=vmap(cost,in_axes=(None,0,0))"
      ],
      "metadata": {
        "id": "f0RiBQSlZNOV"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply this to a batch of three 'training samples':\n",
        "x=jnp.array([[0.3,0.4],[0.9,0.8],[0.1,0.2]]) # batch size of 3, shape (3,2)\n",
        "y_target=jnp.array([[1.0],[0.8],[0.7]]) # shape (3,1)\n",
        "\n",
        "batched_cost(params,x,y_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-te0AhbZvaB",
        "outputId": "73460db2-6576-4e66-f694-1e3d1f6be1f6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([1.        , 0.64000005, 0.48999998], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# want to average the cost over the batch:\n",
        "def average_cost(params,x,y_target):\n",
        "  return jnp.average(batched_cost(params,x,y_target))"
      ],
      "metadata": {
        "id": "jc-Q077jaE2-"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_cost(params,x,y_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45HiMoOCaOR0",
        "outputId": "1c412cab-2dee-45cd-ca6a-b38d9083b0d4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(0.71000004, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batched_value_and_grad_cost=jit(value_and_grad(average_cost))"
      ],
      "metadata": {
        "id": "3OLlC7iUYw05"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "batched_value_and_grad_cost(params,x,y_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnysVAk7ad1R",
        "outputId": "a4c2db9e-85f9-48c7-c391-9ce8e4d1a54d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18.2 µs ± 4.86 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IDLZZ_YNaiWL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}